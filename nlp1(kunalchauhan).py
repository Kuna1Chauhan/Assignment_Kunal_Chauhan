# -*- coding: utf-8 -*-
"""NLP1(KunalChauhan).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12tx2ENBMLrHHW92Izje3UnNf1VhcWymh
"""

import csv
from googleapiclient.discovery import build

# YouTube Data API credentials
API_KEY = 'YOUR_API_KEY'  # Replace with your actual API key

# YouTube video ID
VIDEO_ID = 'ZWxdQDkf_Uw'

# Create YouTube API client
youtube = build('youtube', 'v3', developerKey=API_KEY)

# Retrieve video comments
comments = []
next_page_token = None

while True:
    # Make API request to get comments
    response = youtube.commentThreads().list(
        part='snippet',
        videoId=VIDEO_ID,
        maxResults=100,
        pageToken=next_page_token
    ).execute()

    # Extract comments and replies
    for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
        comments.append(comment)

    # Check if there are more pages of comments
    if 'nextPageToken' in response:
        next_page_token = response['nextPageToken']
    else:
        break

# Store comments in a CSV file
csv_file = 'video_comments.csv'

with open(csv_file, 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Comment'])
    writer.writerows([[comment] for comment in comments])

print('Comments extracted and stored in', csv_file)

# Count occurrences of each word in comments
word_counts = {}

for comment in comments:
    words = comment.lower().split()
    for word in words:
        word_counts[word] = word_counts.get(word, 0) + 1

# Find the most demanding topic
most_demanding_topic = max(word_counts, key=word_counts.get)
demand_count = word_counts[most_demanding_topic]

print('Most demanding topic:', most_demanding_topic)
print('Number of demands for the topic:', demand_count)